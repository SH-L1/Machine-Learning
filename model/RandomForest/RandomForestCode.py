import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# ì½”ë©ìš© í•œê¸€ í°íŠ¸ ì„¤ì •
import matplotlib.font_manager as fm

# ë‚˜ëˆ”ê³ ë”• í°íŠ¸ ì„¤ì¹˜ (ì½”ë©ì—ì„œ)
!apt-get install -y fonts-nanum
!fc-cache -fv
!rm ~/.cache/matplotlib -rf

# í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'NanumBarunGothic'
plt.rcParams['axes.unicode_minus'] = False

# ë§Œì•½ ìœ„ í°íŠ¸ê°€ ì•ˆë˜ë©´ DejaVu Sans ì‚¬ìš©
try:
    plt.rcParams['font.family'] = 'NanumBarunGothic'
except:
    plt.rcParams['font.family'] = 'DejaVu Sans'
    print("í•œê¸€ í°íŠ¸ ì„¤ì • ì‹¤íŒ¨ - ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©")

print(f"í˜„ì¬ í°íŠ¸: {plt.rcParams['font.family']}")


def visualize_manual_tuning_results(results_df):
    param_names = [col for col in results_df.columns if col != 'mean_r2_score']
    for param_name in param_names:
        other_params = [p for p in param_names if p != param_name]
        plt.figure(figsize=(12, 8))
        if other_params:
            grouped = results_df.groupby(other_params)
            for name, group in grouped:
                label = ", ".join([f"{p}={v}" for p, v in zip(other_params, name if isinstance(name, tuple) else (name,))])
                group.sort_values(by=param_name).plot(x=param_name, y='mean_r2_score', ax=plt.gca(), label=label, marker='o', linestyle='-')
        else:
            results_df.sort_values(by=param_name).plot(x=param_name, y='mean_r2_score', ax=plt.gca(), marker='o', linestyle='-')
        plt.title(f'"{param_name}" ë³€í™”ì— ë”°ë¥¸ ëª¨ë¸ ì„±ëŠ¥(R2 Score)', fontsize=16)
        plt.xlabel(param_name)
        plt.ylabel('í‰ê·  êµì°¨ê²€ì¦ ì ìˆ˜ (R2)')
        plt.grid(True)
        plt.legend(title="ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° ì¡°í•©")
        plt.tight_layout()
        plt.savefig(f'manual_tuning_lineplot_{param_name}.png')

    print(f"\ní•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ ë¼ì¸ ê·¸ë˜í”„ {len(param_names)}ê°œê°€ .png íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def train_final_model():
    csv_path = '/content/drive/MyDrive/ì°ë§‰ë°ì´í„°_2 (1).csv'
    try:
        data = pd.read_csv(csv_path, encoding='cp949')
    except Exception as e:
        print(f"ì˜¤ë¥˜: CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ - {e}")
        return

    categorical_features = ['í´ëŸ¬ìŠ¤í„°', 'ì—°ë„_ë¶„ê¸°']
    numerical_features = [f'{i}ì‹œ' for i in range(24)] + ['í‰ê· ê¸°ì˜¨(â„ƒ)', 'ê°•ìˆ˜ëŸ‰(mm)', 'ê³µíœ´ì¼ ìˆ˜']

    target_cols = [
        'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡', 'ë‹¹ì›”_ë§¤ì¶œ_ê±´ìˆ˜', 'ì£¼ì¤‘_ë§¤ì¶œ_ê¸ˆì•¡', 'ì£¼ë§_ë§¤ì¶œ_ê¸ˆì•¡', 'ì›”ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡', 'í™”ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡',
        'ìˆ˜ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡', 'ëª©ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡', 'ê¸ˆìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡', 'í† ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡', 'ì¼ìš”ì¼_ë§¤ì¶œ_ê¸ˆì•¡', 'ì‹œê°„ëŒ€_00~06_ë§¤ì¶œ_ê¸ˆì•¡',
        'ì‹œê°„ëŒ€_06~11_ë§¤ì¶œ_ê¸ˆì•¡', 'ì‹œê°„ëŒ€_11~14_ë§¤ì¶œ_ê¸ˆì•¡', 'ì‹œê°„ëŒ€_14~17_ë§¤ì¶œ_ê¸ˆì•¡', 'ì‹œê°„ëŒ€_17~21_ë§¤ì¶œ_ê¸ˆì•¡', 'ì‹œê°„ëŒ€_21~24_ë§¤ì¶œ_ê¸ˆì•¡',
        'ë‚¨ì„±_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—¬ì„±_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_10_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_40_ë§¤ì¶œ_ê¸ˆì•¡',
        'ì—°ë ¹ëŒ€_50_ë§¤ì¶œ_ê¸ˆì•¡', 'ì—°ë ¹ëŒ€_60_ì´ìƒ_ë§¤ì¶œ_ê¸ˆì•¡', 'ì£¼ì¤‘_ë§¤ì¶œ_ê±´ìˆ˜', 'ì£¼ë§_ë§¤ì¶œ_ê±´ìˆ˜', 'ì›”ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜', 'í™”ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜',
        'ìˆ˜ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜', 'ëª©ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜', 'ê¸ˆìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜', 'í† ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜', 'ì¼ìš”ì¼_ë§¤ì¶œ_ê±´ìˆ˜', 'ì‹œê°„ëŒ€_ê±´ìˆ˜~06_ë§¤ì¶œ_ê±´ìˆ˜',
        'ì‹œê°„ëŒ€_ê±´ìˆ˜~11_ë§¤ì¶œ_ê±´ìˆ˜', 'ì‹œê°„ëŒ€_ê±´ìˆ˜~14_ë§¤ì¶œ_ê±´ìˆ˜', 'ì‹œê°„ëŒ€_ê±´ìˆ˜~17_ë§¤ì¶œ_ê±´ìˆ˜', 'ì‹œê°„ëŒ€_ê±´ìˆ˜~21_ë§¤ì¶œ_ê±´ìˆ˜', 'ì‹œê°„ëŒ€_ê±´ìˆ˜~24_ë§¤ì¶œ_ê±´ìˆ˜',
        'ë‚¨ì„±_ë§¤ì¶œ_ê±´ìˆ˜', 'ì—¬ì„±_ë§¤ì¶œ_ê±´ìˆ˜', 'ì—°ë ¹ëŒ€_10_ë§¤ì¶œ_ê±´ìˆ˜', 'ì—°ë ¹ëŒ€_20_ë§¤ì¶œ_ê±´ìˆ˜', 'ì—°ë ¹ëŒ€_30_ë§¤ì¶œ_ê±´ìˆ˜', 'ì—°ë ¹ëŒ€_40_ë§¤ì¶œ_ê±´ìˆ˜',
        'ì—°ë ¹ëŒ€_50_ë§¤ì¶œ_ê±´ìˆ˜', 'ì—°ë ¹ëŒ€_60_ì´ìƒ_ë§¤ì¶œ_ê±´ìˆ˜'
    ]

    final_categorical_features = [col for col in categorical_features if col in data.columns]
    final_numerical_features = [col for col in numerical_features if col in data.columns]
    final_target_cols = [col for col in target_cols if col in data.columns]
    required_feature_cols = final_categorical_features + final_numerical_features
    
    # ê²°ì¸¡ê°’ í™•ì¸ ë° ì²˜ë¦¬
    print(f"ì „ì²´ ë°ì´í„° í¬ê¸°: {data.shape}")
    print(f"ê²°ì¸¡ê°’ ìˆëŠ” í–‰ ìˆ˜: {data[required_feature_cols + final_target_cols].isnull().any(axis=1).sum()}")
    
    data.dropna(subset=required_feature_cols + final_target_cols, inplace=True)
    print(f"ê²°ì¸¡ê°’ ì œê±° í›„ ë°ì´í„° í¬ê¸°: {data.shape}")

    if 'Unnamed: 0' in data.columns:
        data.drop(columns=['Unnamed: 0'], inplace=True)

    X = data[required_feature_cols]
    y = data[final_target_cols]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    preprocessor = ColumnTransformer(transformers=[
        ('num', StandardScaler(), final_numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), final_categorical_features)
    ])

    # RandomForest íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ - None ëŒ€ì‹  ì •ìˆ˜ ê°’ ì‚¬ìš©
    param_grid = {
        'n_estimators': [100, 200, 300],
        'max_depth': [5, 10, 15, 20],  # None ëŒ€ì‹  í° ê°’ë“¤ ì‚¬ìš©
        'max_features': ['sqrt', 'log2']
    }

    primary_target = 'ë‹¹ì›”_ë§¤ì¶œ_ê¸ˆì•¡'
    tuning_results = []
    kf = KFold(n_splits=3, shuffle=True, random_state=42)

    print("í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘...")
    total_combinations = np.prod([len(v) for v in param_grid.values()])
    count = 0
    
    for n in param_grid['n_estimators']:
        for d in param_grid['max_depth']:
            for f in param_grid['max_features']:
                count += 1
                scores = []
                
                # íŒŒë¼ë¯¸í„° ê°’ ê²€ì¦
                print(f"í˜„ì¬ íŒŒë¼ë¯¸í„°: n_estimators={n}, max_depth={d}, max_features={f}")
                
                # max_depthê°€ Noneì´ê±°ë‚˜ nanì¸ì§€ í™•ì¸
                if pd.isna(d) or d is None:
                    print(f"ê²½ê³ : max_depth ê°’ì´ ì˜ëª»ë¨: {d}")
                    continue
                
                for train_idx, val_idx in kf.split(X_train):
                    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
                    y_tr, y_val = y_train[primary_target].iloc[train_idx], y_train[primary_target].iloc[val_idx]

                    try:
                        model = RandomForestRegressor(
                            n_estimators=int(n), 
                            max_depth=int(d) if d is not None else None, 
                            max_features=f, 
                            random_state=42, 
                            n_jobs=-1
                        )
                        pipeline = Pipeline([
                            ('preprocessor', preprocessor),
                            ('regressor', model)
                        ])
                        pipeline.fit(X_tr, y_tr)
                        score = pipeline.score(X_val, y_val)
                        scores.append(score)
                    except Exception as e:
                        print(f"ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue

                if scores:  # scoresê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ
                    mean_score = np.mean(scores)
                    tuning_results.append({
                        'n_estimators': n,
                        'max_depth': d,
                        'max_features': f,
                        'mean_r2_score': mean_score
                    })
                    print(f"[{count}/{total_combinations}] n={n}, depth={d}, feat={f} => R2={mean_score:.4f}")

    if not tuning_results:
        print("í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        best_params = {'n_estimators': 100, 'max_depth': 10, 'max_features': 'sqrt'}
    else:
        results_df = pd.DataFrame(tuning_results)
        best_row = results_df.loc[results_df['mean_r2_score'].idxmax()]
        best_params = best_row.drop('mean_r2_score').to_dict()

        print("\nâœ… ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©:")
        print(best_params)
        print(f"ìµœì  ì¡°í•© R2 Score: {best_row['mean_r2_score']:.4f}")

        visualize_manual_tuning_results(results_df)

    # ìµœì¢… ëª¨ë¸ í•™ìŠµ
    final_model = RandomForestRegressor(
        n_estimators=int(best_params['n_estimators']),
        max_depth=int(best_params['max_depth']),
        max_features=best_params['max_features'],
        random_state=42, 
        n_jobs=-1
    )
    
    final_pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', MultiOutputRegressor(final_model))
    ])
    
    final_pipeline.fit(X_train, y_train)
    y_pred = final_pipeline.predict(X_test)

    # ì„±ëŠ¥ í‰ê°€
    metrics_df = pd.DataFrame(columns=['MSE', 'RMSE', 'MAE', 'R2 Score'])
    for i, target_name in enumerate(final_target_cols):
        mse = mean_squared_error(y_test.iloc[:, i], y_pred[:, i])
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_test.iloc[:, i], y_pred[:, i])
        r2 = r2_score(y_test.iloc[:, i], y_pred[:, i])
        metrics_df.loc[target_name] = [mse, rmse, mae, r2]

    print("\nğŸ“Š ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€:")
    with pd.option_context('display.float_format', '{:,.2f}'.format):
        print(metrics_df)

    return final_pipeline, metrics_df

if __name__ == '__main__':
    model, metrics = train_final_model()
    if model:
        print("\nëª¨ë¸ í•™ìŠµ ì™„ë£Œ âœ…")
    else:
        print("\nëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨ âŒ")
